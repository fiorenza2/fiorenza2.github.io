---
layout: post
title: "Notes on: Deep Frank-Wolfe For Neural Network Optimization"
date: 2018-12-01
mathjax: true
---

Original Paper by: Leonard Berrada, Andrew Zisserman, M. Pawan Kumar

# High Level Overview

1. When we do standard gradient descent, we are in fact solving a closed form solution of a 1st-order Taylor Series Proximal problem (see \ref{eq:taylor_proximal_full}). We can improve the accuracy of this approximation by pushing the Taylor series approximation into the the loss function (see \ref{eq:loss_preserve}).
2. This improvement on the approximation can be made tractable by selecting a piecewise-linear loss (i.e., multi-class hinge, L1), and it can be shown that the cost of such updates is equivalent to SGD.
3. Furthermore, we can formulate the problem in order to determine the optimal stepsize ($\gamma$) to this proximal approximation; we do this by rewriting the primal objective in its (strong) dual objective.
4. We can use the Franke-Wolfe (FW) algorithm to solve this to find $\gamma$. Since we are still using an approximation (see the $\mathcal{T}$ in \ref{eq:loss_preserve}) to the true proximal solution, we need to only iterate FW once, since by iterating more than once we'd be getting closer to an approximation anyway, which is arguably not helpfuluse.

# Introduction

# Motivation

$$
\begin{equation}
\mathbf{w}_{t+1} = \underset{\mathbf{w}\in\mathbb{R}^p}{\arg\min} \left\{ \frac{1}{2 \nu_t} \|\mathbf{w}-\mathbf{w_t}\|^2 + \mathcal{T}_{\mathbf{w}_t}\rho(\mathbf{w}) + \mathcal{T}_{\mathbf{w}_t} [ \mathcal{L}_j(\mathbf{f}_j(\mathbf{w}))] \right\}
\label{eq:taylor_proximal_full}
\end{equation}
$$

$$
\begin{equation}
\mathbf{w}_{t+1} = \underset{\mathbf{w}\in\mathbb{R}^p}{\arg\min} \left\{ \frac{1}{2 \nu_t} \|\mathbf{w}-\mathbf{w_t}\|^2 + \mathcal{T}_{\mathbf{w}_{t}}\rho(\mathbf{w}) + \mathcal{L}_{j} ( \mathcal{T}_{\mathbf{w}_t}\mathbf{f}_j(\mathbf{w}) ) \right\}
\label{eq:loss_preserve}
\end{equation}
$$