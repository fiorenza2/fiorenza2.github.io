---
layout: post
title: "Notes on: Deep Frank-Wolfe For Neural Network Optimization"
date: 2018-12-01
mathjax: true
---

by: Leonard Berrada, Andrew Zisserman, M. Pawan Kumar

# High Level Overview

1. When we do standard gradient descent, we are in fact 
    1. Creating a 1st-order Taylor Series approximation of the following equation:

2. Hello
3. Bye

\begin{equation}
\mathbf{w}_{t+1} = \underset{\mathbf{w}\in\mathbb{R}^p}{\arg\min} \left\{ \frac{1}{2 \nu_t} \|\mathbf{w}-\mathbf{w_t}\|^2 + \mathcal{T}_{\mathbf{w}_t}\rho(\mathbf{w}) + \mathcal{T}_{\mathbf{w}_t} [ \mathcal{L}_j(\mathbf{f}_j(\mathbf{w}))] \right\}
\label{eq:taylor_proximal_full}
\end{equation}

$$\mathbf{w}_{t+1} = \underset{\mathbf{w}\in\mathbb{R}^p}{\arg\min} \left\{ \frac{1}{2 \nu_t} \|\mathbf{w}-\mathbf{w_t}\|^2 + \mathcal{T}_{\mathbf{w}_t}\rho(\mathbf{w}) + \mathcal{T}_{\mathbf{w}_t} [ \mathcal{L}_j(\mathbf{f}_j(\mathbf{w}))] \right\}
\tag{1}
\label{eq:taylor_proximal_full}
$$