---
layout: post
title: "Reinforcement Learning: Introductory Notes"
date: 2019-07-08
mathjax: true
---

## Introduction 
Reinforcement learning aims to solve finite Markov decision processes (MDP). This is done by learning the optimal policy given a state. It may be necessary to model the hidden state explicitely, since we may only have access to some imperfect information regarding the true state, a scenario which is described in the \ref{sec:mdp}.

## Markov Decision Processes
Both active inference and reinforcement learning can be used to solve using MDPs, or more specifically POMDPs in the case of active inference. Given this common goal, we describe what is meant by MDPs and POMDPs.

$$
\begin{definition}[Markov Decision Process]
An MDP is defined as a 5-tuple of $(S^e,A,P_{S'},R,\gamma)$, where:
\begin{itemize}
    \item $S^e$: The set of environment states
    \item $A$: The set of actions the agent can take
    \item $P_{S'}(s'|s,a)$: The conditional probability distribution of next states $s'$ given the action $a$ and previous state $s$
    \item $R(s',s,a)$: The reward function for transitioning to state $s'$ given action $a$ and previous state $s$ 
    \item $\gamma$: The discount factor on future rewards
\end{itemize}
\end{definition}
$$
where all states in the transition and reward functions are $s\in S^e$.

Such a process is defined as Markovian as it possesses the Markovian property, such that all ``the future is independent of the past given the present". Concretely, the following holds:
\begin{align}
    \mathbb{P}[S^e_{t+1}|S^e_t] = \mathbb{P}[S^e_{t+1}|S^e_1,\dots,S^e_t]
\end{align}

In other words, once the state is known, all history up to that point may be thrown away. This is not the same as saying that the process is history-less; instead if the history affects subsequent states, it will be stored in some form within the current state $S^e_t$.

Also of note is the discounting of future rewards, represented by $\gamma$. One perspective on discounting is that generally, we are less certain about actions and rewards in the future, therefore we wish to discount these future rewards with respect to more recent ones. Another interpretation, specifically in the context of infinite-horizon MDPs (i.e., MDPs that have no terminating state(s)) is that $1-\gamma$ represents the probability that the environment may randomly terminate after each step. In this framing, $\gamma$ is a property of the environment, hence its inclusion in the tuple.

Generally an agent does not have access to the true state of the environment $S^e$; instead it has access to an observation $O^e$, which provides a `partial' view upon the true state $S^e$. Sometimes it is sufficient to say that $S^e \equiv O^e$, which assumes that the observations provide perfect knowledge of the state. However such environments are relatively trivial and are therefore not representative of most real-life scenarios (i.e., consider a 3D world where objects of interest can become occluded in a visual input). Instead, an additional set of variables must be included on top of the existing MDP definition.

\begin{definition}[Partially Observable Markov Decision Process]
A POMDP is defined as a 7-tuple of $(S^e,O^e,A,P_{S'},P_{O},R,\gamma)$, where:
\begin{itemize}
    \item $S^e$: The set of environment states
    \item $O^e$: The set of environment observations
    \item $A$: The set of actions the agent can take
    \item $P_{S'}(s'|s,a)$: The conditional probability distribution of next states $s'$ given the action $a$ and previous state $s$
    \item $P_{O}(o|s',a)$: The conditional probability distribution of an observation $o$ given the action $a$ and state $s'$
    \item $R(s',s,a)$: The reward function of transitioning to state $s'$ given action $a$ and previous state $s$ 
    \item $\gamma$: The discount factor on future rewards
\end{itemize}
\end{definition}

The POMDP is much more difficult to solve for due to the uncertainty an agent has over the true state; if an agent chooses to use only the observation $O^e$ to make decisions over actions, it clearly loses information concerning the environment, and therefore is unlikely to behave optimally. Therefore it is required that the agent constructs for itself an internal state, or belief, on which it can make decisions. Without loss of generality, it can be stated that the state of the POMDP environment at any time is some function of the history:
\begin{align}
    S^e_t &= f(O_1,R_1,A_1,\dots,A_{t-1},O_t,R_t)\\
        &=  f(H_t).
\end{align}
Therefore one approach to compute the internal agent state $S^a_t$ would be to learn some sort of function via a state estimator (SE) that maps the history to a belief:
\begin{align}
    S^a_t &= f_{SE}(O_1,R_1,A_1,\dots,A_{t-1},O_t,R_t)\\
        &=  f_{SE}(H_t)\\
        &\triangleq b_t.
\end{align}

\begin{figure}[H]
\center{\includegraphics[width=0.5\textwidth]
{figures/pomdp.png}}
\caption{\label{fig:my-label} Illustration of the POMDP (from \cite{kaelbling1998}), where SE stands for state estimator}
\end{figure}

This gives a notion of how an agent is able to learn some internal representation/belief of the environment state upon which it can make decisions (i.e., using a particle filter \cite{igl2018}). If the agent is able to learn an appropriate belief over states given observations, the POMDP is effectively reduced to that of the MDP problem \cite{kaelbling1998}. As will be illustrated in subsequent sections, active inference has a natural way of inferring beliefs over states through generative modelling, and this distinguishes it from the traditional reinforcement learning setup.

This is a generic formulation of POMDPs, however within active inference there is no explicit requirement of a reward signal, and by extension discount factor \cite{friston2012}. How active inference encourages optimal behaviour is discussed in Section \ref{sec:activeinference}.

\subsection{Reinforcement Learning}

Reinforcement learning is a branch of machine learning that aims to optimise an agent's behaviour in MDPs such that some numerical signal is maximised \cite{suttonbarto}.

In order to define the agent's behaviour, we must have some way of mapping states to actions. This mapping is the policy $\pi$:
\begin{definition}
    A policy is defined as the following distribution over actions:
        \begin{align}
            \pi(a|s) = \mathbb{P}[A_t=a|S^a_t=s]
        \end{align}
\end{definition}
where $S^a_t$ is the agent's internal representation of the state. In much recent reinforcement learning literature, full observability is assumed such that the equality $S^e_t = O_t$ is valid (sometimes a simple data transformation is applied to observations to enable this, such as stacking frames \cite{mnih2015}, giving $S^e_t = f(O_{t-3},\dots,O_{t})$). This can give a misconception that RL only works well with MDPs where states that are fully observable, but this is not the case, with discussion in Section \ref{sec:beliefbasefree} showing how various techniques can be applied to RL algorithms such that they can learn complex beliefs over the true states of the environment. For now we assume that the agent can access the true state $S^e_t$.

We have mentioned optimality, and it is defined as behaviour that maximises some return $G$:
\begin{definition}
In an infinite horizon MDP, return $G$ at some time step $t$ is defined as:
    \begin{align}
        G_t &\triangleq \gamma^0 R_{t+1} + \gamma^1 R_{t+2} + \gamma^2 R_{t+3} + \dots \\
            &= \sum_{k=0}^\infty \gamma^k R_{t+k+1}.
    \end{align}
\end{definition}

The process of finding some policy $\pi$ that maximises $G_t$ is still incomplete; after all, we need some way to estimate the return $G_t$ at any given point in the state space $S_t$. This allows us to make decisions over which actions to take such that return is maximised (and thus optimal).

In order to estimate $G_t$ we introduce two important functions within the RL literature; the state-value function $v_\pi$ and the action-value function $q_\pi$ (collectively referred to as value functions):
\begin{definition}[State-Value function]
    The state-value function is defined as the expected return from following a policy $\pi$ starting from state $s$,
    \begin{align}\label{eq:sv-func}
        v_\pi(s) &= \mathbb{E}[G_t|S_t=s].
    \end{align}
\end{definition}
\begin{definition}[Action-Value function]
    The action-value function is defined as the expected return starting from state $s$, taking action $a$, and subsequently following a policy $\pi$,
    \begin{align}\label{eq:av-func}
        q_\pi(s,a) &= \mathbb{E}[G_t|S_t=s,A_t=a].
    \end{align}
\end{definition}

We now introduce the notion of optimal agent behaviour with respect to the value functions:
\begin{definition}[Optimal value functions]
    The optimal state-value function is the maximum state-value function over all policies
    \begin{align}\label{eq:opt-sv}
        v_*(s) = \underset{\pi}{\max}\ v_\pi(s) \quad \forall s \in S^e.
    \end{align}
    
    The optimal action-value function is the maximum action-value function over all policies
    \begin{align}\label{eq:opt-av}
        q_*(s,a) = \underset{\pi}{\max}\ q_\pi(s,a)
    \end{align}
\end{definition}

There is a duality here; if we have access to the optimal policy, we can determine the optimal value functions, and vice-versa. To obtain the optimal policy given the optimal action-value function\footnote{We can write the optimal action-value function in terms of the optimal state-value function: $q_*(s,a) = R_a(s,s') + \gamma\mathbb{E}_{s'\sim P_a}[v_*(s')] $}, we simply maximise actions over the action-value function:
\begin{align}\label{eq:optimalpolicy}
    \pi_*(a|s) = \begin{cases}
    1 \quad \text{if}\ a = \underset{a}{\arg\max}\ q_*(s,a)\\
    0 \quad \text{otherwise}
    \end{cases}
\end{align}

However this does not give us a method by which we can actually calculate the optimal value functions or policies, it only tells us a property that such functions must have.

One way to obtain the optimal value/policy functions is by invoking the Bellman equations for MDPs. This approach is the one used by Temporal Difference (TD) methods, and gives a way to design an update rule so that optimality can be determined. Note that the Bellman equations are Eqs \ref{eq:sv-func} and \ref{eq:av-func} decomposed into an immediate reward and all subsequent returns.

\begin{definition}[Bellman Equations]
    The Bellman equation for the state-value function is defined as
    \begin{align}
        v_\pi(s) = \mathbb{E}_{a\sim \pi}[R_a(s,s') + \gamma \mathbb{E}_{s'\sim P_a}[v_\pi(s')]]
    \end{align}
    
    The Bellman equation for the action-value function is defined as
    \begin{align}
        q_\pi(s,a) = R_a(s,s') + \gamma \mathbb{E}_{s'\sim P_a}\mathbb{E}_{a'\sim \pi}[q_\pi(s',a')]
    \end{align}
    
\end{definition}
where $s',a'$ are the state and action respectively at the next time step.

We can combine this with Eqs \ref{eq:opt-sv} and \ref{eq:opt-av} to write the Bellman Optimality Conditions.

\begin{definition}[Bellman Optimality Conditions]\label{def:bellmanopt}
    For a state-value function to be optimal, the following condition must hold
    \begin{align}
        v_*(s) = \underset{a}{\max}\ R_a(s,s') + \gamma\mathbb{E}_{s'\sim P_a}[v_*(s')]
    \end{align}
    
    For an action-value function to be optimal, the following condition must hold
    \begin{align}
        q_*(s,a) = R_a(s,s') + \gamma \mathbb{E}_{s'\sim P_a}[\underset{a'}{\max}\ q_*(s',a')]
    \end{align}
\end{definition}

Generally solutions to the above functional equations have no closed-form, therefore iterative methods are required. The link between Definition \ref{def:bellmanopt} and an actual RL algorithm using TD methods is made explicit in the pseudocode for Algorithm \ref{alg:TD} in Appendix \ref{appendix:pseudocode}; in essence, the equalities are turned into assignments, and are updated iteratively.\phil{add reference}

On a final note, we have made explicit mention to TD methods for solving MDPs using RL, but other approaches exist, such as directly estimating the policy $\pi$ by parameterising it and maximising expected return (i.e., policy gradients \cite{sutton1999}), or a hybrid of the two approaches (i.e., actor-critic \cite{konda1999}). Pseudo code for two such approaches is given in Appendix \ref{appendix:pseudocode}.