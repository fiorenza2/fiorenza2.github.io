---
layout: post
title: "Forward vs Reverse KL Divergences: Implementation Notes"
date: 2019-03-29
mathjax: true
published: false
---

# Introduction

We wish to minimise the reverse or forward KL divergences between two distributions, namely 

1. A bi-modal Gaussian $$P$$ acting as the 'actual' distribution
2. A uni-modal Gaussian $$Q$$ acting as the 'approximating' distribution

# Forward KL

This is straightforward; we can use Monte-Carlo methods to approximate the required parameters for $$Q$$. First, we rewrite the KL Divergence as a MC integral:

$$
\begin{align}
D_{\text{KL}}(P || Q) &= \int P(x) \log \frac{P(x)}{Q(x)} \text{d}x \\
&= \frac{1}{n} \sum_{i=1}^\infty \log P(x) - \log Q(x)
\label{eq:MCApprox}
\end{align}
$$

We 